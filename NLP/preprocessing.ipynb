{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589b0d85",
   "metadata": {},
   "source": [
    "Case folding is a technique that we use to reduce case insensivity, normalizing text and enhace consistency in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e63e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, And Welcome to my world\n"
     ]
    }
   ],
   "source": [
    "txt = \"Hello, And Welcome to my world\"\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60ddb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, and welcome to my world\n"
     ]
    }
   ],
   "source": [
    "x = txt.casefold()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221e0a1",
   "metadata": {},
   "source": [
    "casefold is mainly lowercasing the text for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2aa90f",
   "metadata": {},
   "source": [
    "We need to remove the special characters like $,etc for noise elimation, improve text quality and better tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9dc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#input string\n",
    "input_str = \"Hello how are you$!!\"\n",
    "\n",
    "#clean the string\n",
    "clean_str = re.sub(r\"[^a-zA-Z0-9\\s]\",\"\",input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e97f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you\n"
     ]
    }
   ],
   "source": [
    "print(clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1f0d5",
   "metadata": {},
   "source": [
    "## Use libraries:\n",
    "- NLTK\n",
    "- SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e700eee",
   "metadata": {},
   "source": [
    "download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f841a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696c230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#load model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#input str\n",
    "input_str = \"Hello how are you$!!\"\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace()) #filter out the unneeded character and space\n",
    "    doc = nlp(cleaned_text)\n",
    "    return ' '.join(token.text for token in doc) #add the spaces to the beginning text\n",
    "\n",
    "clean_str = clean_text(input_str)\n",
    "print(clean_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "963ed7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/mymac/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdd11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you\n"
     ]
    }
   ],
   "source": [
    "input_str = \"Hello how are you$!!\"\n",
    "# Tokenize the input\n",
    "tokens = nltk.word_tokenize(input_str)\n",
    "# If the token is alpha or num then we use it\n",
    "clean_tokens = [token for token in tokens if token.isalnum()]\n",
    "# add spaces\n",
    "clean_str = \" \".join(clean_tokens)\n",
    "\n",
    "print(clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f914b0",
   "metadata": {},
   "source": [
    "Handling contractions:\n",
    "\n",
    "Ex: isn't -> is not\n",
    "\n",
    "We can use the contraction library for handling this contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72ae12b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot believe that I am the winner.\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "txt = \"I can't believe that I'm the winner.\"\n",
    "\n",
    "expanded_txt = contractions.fix(txt)\n",
    "\n",
    "print(expanded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot believe that I am the winner.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def expand_contractions(text):\n",
    "    contractions_pattern = {\n",
    "        r\"(?i)can't\": \"cannot\",\n",
    "        r\"(?i)isn't\": \"is not\",   #(?i) is for the case-sensitive, which mean it works for both lowercase and uppercase\n",
    "        r\"(?i)aren't\": \"are not\",\n",
    "        r\"(?i)won't\": \"will not\",\n",
    "        r\"(?i)weren't\": \"were not\",\n",
    "        r\"(?i)I'm\": \"I am\"\n",
    "    }\n",
    "    for contraction, expansion in contractions_pattern.items():\n",
    "        text = re.sub(contraction,expansion,text)\n",
    "    return text\n",
    "\n",
    "txt = \"I can't believe that I'm the winner.\"\n",
    "print(expand_contractions(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863cab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
