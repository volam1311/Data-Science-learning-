{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589b0d85",
   "metadata": {},
   "source": [
    "Case folding is a technique that we use to reduce case insensivity, normalizing text and enhace consistency in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e63e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, And Welcome to my world\n"
     ]
    }
   ],
   "source": [
    "txt = \"Hello, And Welcome to my world\"\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60ddb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, and welcome to my world\n"
     ]
    }
   ],
   "source": [
    "x = txt.casefold()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221e0a1",
   "metadata": {},
   "source": [
    "casefold is mainly lowercasing the text for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2aa90f",
   "metadata": {},
   "source": [
    "We need to remove the special characters like $,etc for noise elimation, improve text quality and better tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9dc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#input string\n",
    "input_str = \"Hello how are you$!!\"\n",
    "\n",
    "#clean the string\n",
    "clean_str = re.sub(r\"[^a-zA-Z0-9\\s]\",\"\",input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e97f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you\n"
     ]
    }
   ],
   "source": [
    "print(clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1f0d5",
   "metadata": {},
   "source": [
    "## Use libraries:\n",
    "- NLTK\n",
    "- SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e700eee",
   "metadata": {},
   "source": [
    "download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f841a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696c230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#load model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#input str\n",
    "input_str = \"Hello how are you$!!\"\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace()) #filter out the unneeded character and space\n",
    "    doc = nlp(cleaned_text)\n",
    "    return ' '.join(token.text for token in doc) #add the spaces to the beginning text\n",
    "\n",
    "clean_str = clean_text(input_str)\n",
    "print(clean_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "963ed7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/mymac/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdd11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you\n"
     ]
    }
   ],
   "source": [
    "input_str = \"Hello how are you$!!\"\n",
    "# Tokenize the input\n",
    "tokens = nltk.word_tokenize(input_str)\n",
    "# If the token is alpha or num then we use it\n",
    "clean_tokens = [token for token in tokens if token.isalnum()]\n",
    "# add spaces\n",
    "clean_str = \" \".join(clean_tokens)\n",
    "\n",
    "print(clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f914b0",
   "metadata": {},
   "source": [
    "Handling contractions:\n",
    "\n",
    "Ex: isn't -> is not\n",
    "\n",
    "We can use the contraction library for handling this contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72ae12b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot believe that I am the winner.\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "txt = \"I can't believe that I'm the winner.\"\n",
    "\n",
    "expanded_txt = contractions.fix(txt)\n",
    "\n",
    "print(expanded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot believe that I am the winner.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def expand_contractions(text):\n",
    "    contractions_pattern = {\n",
    "        r\"(?i)can't\": \"cannot\",\n",
    "        r\"(?i)isn't\": \"is not\",   #(?i) is for the case-sensitive, which mean it works for both lowercase and uppercase\n",
    "        r\"(?i)aren't\": \"are not\",\n",
    "        r\"(?i)won't\": \"will not\",\n",
    "        r\"(?i)weren't\": \"were not\",\n",
    "        r\"(?i)I'm\": \"I am\"\n",
    "    }\n",
    "    for contraction, expansion in contractions_pattern.items():\n",
    "        text = re.sub(contraction,expansion,text)\n",
    "    return text\n",
    "\n",
    "txt = \"I can't believe that I'm the winner.\"\n",
    "print(expand_contractions(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0f7b0",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4863cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2d2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lam', 'is', 'so', 'handsome', '.', 'He', 'is', 'gon', 'na', 'be', 'a', 'coder', '.']\n"
     ]
    }
   ],
   "source": [
    "sample = \"Lam is so handsome. He is gonna be a coder.\"\n",
    "words = word_tokenize(sample)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248d8de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lam is so handsome.', 'He is gonna be a coder.']\n"
     ]
    }
   ],
   "source": [
    "setences = sent_tokenize(sample)\n",
    "print(setences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56ce68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mymac/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33061fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is a sample sentence, showing off my technical skills.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a9c821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', ',', 'showing', 'technical', 'skills', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "words = word_tokenize(sentence)\n",
    "new_sentence = [word for word in words if word.lower() not in stop_words]\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eab1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e168751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(text,n):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = list(ngrams(tokens,n))\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29756de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('N-Grams',), ('is',), ('a',), ('high',), ('technique',), ('that',), ('you',), ('should',), ('learn',), ('in',), ('Artificial',), ('Intelligence',), ('.',)]\n",
      "[('N-Grams', 'is'), ('is', 'a'), ('a', 'high'), ('high', 'technique'), ('technique', 'that'), ('that', 'you'), ('you', 'should'), ('should', 'learn'), ('learn', 'in'), ('in', 'Artificial'), ('Artificial', 'Intelligence'), ('Intelligence', '.')]\n",
      "[('N-Grams', 'is', 'a'), ('is', 'a', 'high'), ('a', 'high', 'technique'), ('high', 'technique', 'that'), ('technique', 'that', 'you'), ('that', 'you', 'should'), ('you', 'should', 'learn'), ('should', 'learn', 'in'), ('learn', 'in', 'Artificial'), ('in', 'Artificial', 'Intelligence'), ('Artificial', 'Intelligence', '.')]\n"
     ]
    }
   ],
   "source": [
    "txt = \"N-Grams is a high technique that you should learn in Artificial Intelligence.\"\n",
    "unigrams = generate_tokens(txt,1)\n",
    "bigrams = generate_tokens(txt,2)\n",
    "trigrams = generate_tokens(txt,3)\n",
    "print(unigrams)\n",
    "print(bigrams)\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb2a0d",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854b36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
